# Packages und Vorbereitung ----
# Alle Analysen wurden mit R-Version 4.0.2 und RStudio 1.3.959 durchgef√ºhrt. Verwendete Packages werden hier gesammelt gelistet:

packages <- c("dplyr", "readr", "tidyr", "stringi", "stringr", "tibble", "ggplot2", "reshape2", "corrplot", "cowplot", "lubridate", "magrittr", "tm", "stm", "RColorBrewer")
for (pkg in packages) {
  if (pkg %in% rownames(installed.packages()) == FALSE)
  {install.packages(pkg)}
}
rm(packages, pkg)

{
  library(dplyr)
  library(readr)
  library(tibble)
  library(ggplot2)
  library(reshape2)
  library(RColorBrewer)
  library(corrplot)
  library(cowplot)
  library(tidyr)
  library(stringi)
  library(stringr)
  library(lubridate)
  library(quanteda)
  library(magrittr)
  library(tm)
  library(stm)
  
  set.seed(2020)
  setwd("Y:\\Twitter Bachelor")
}

### Der in dieser Datei pr√§sentierte Code ist als konstant durchlaufendes Script gedacht - Vollst√§ndiges Markieren und Ausf√ºhren ist also m√∂glich, wird aber aufgrund der voraussichtlichen Rechenzeit nicht angeraten. Da einige der im Folgenden erzeugen Dateien aus aufw√§ndigen und/oder rechenintensiven Schritten entstehen, besteht die M√∂glichkeit, diese komplexeren Elemente direkt zu laden. Aus diesem Grund werden sich an einzelnen Punkten in auskommentierter Form die Codes zum Speichern und Laden von Workspace-Dateien der jeweils erzeugten Daten finden.
# Sollte man nach einer bestimmten Datei suchen, oder einen √ºberblick √ºber alle verf√ºgbaren Workspace-Elemente haben wollen, so findet sich im zweiten R-Script ("Code - Data Loading.R") der Speicher- und Ladecode geb√ºndelt und in √ºbersichtlicher Form.



# Verwendete Twitter-Datens√§tze ----
users <- read_csv("Twitter Data/ira_users_csv_hashed.csv") # Datensatz der Nutzer, Version vom 05.02.2019 (aktuellste Version, Stand Juli 2020)

tweets <- read_csv("Twitter Data/ira_tweets_csv_hashed.csv", col_types = cols(tweetid = col_character(), retweet_tweetid = col_character(), in_reply_to_tweetid = col_character(), latitude = col_factor(), longitude = col_factor(), poll_choices = col_character())) # Datensatz der Tweets, Version vom 11.02.2019 (aktuellste Version, Stand Juli 2020)

#tweets_oldver <- read_csv("Twitter Data/ira_tweets_csv_hashed_alt.csv", col_types = cols(tweetid = col_character(), retweet_tweetid = col_character(), in_reply_to_tweetid = col_character(), latitude = col_character(), longitude = col_character(), poll_choices = col_character())) # Datensatz der Tweets, Version vom 15.10.2018

head(as_tibble(tweets), n=20)
# Nutzung von read_csv (readr) statt read.csv (base), da base-Funktion ohne gro√üen Aufwand nicht zur Darstellung der unterschiedlichen Schrifts√§tze (westlich, kyrillisch, arabisch, ...) f√§hig zu sein scheint.

# !ACHTUNG!
# Es ist aufgrund der schieren Gr√∂√üe der Tweet-Datens√§tze (jeweils >5GB) je nach vorhandenem Arbeitsspeichers dazu zu raten, diese nur dann zu laden, wenn sie aktiv ben√∂tigt werden und nach Gebrauch zu entladen ( rm(NAME) ). Sollte dies den Speicher nicht komplett leeren, kann mit Garbage Collection oder R-Neustart nachgeholfen werden ( gc() bzw. .rs.restartR() respektive).



# Analyse Users: ----

### Account-Sprachen
languages <- data.frame(sort(table(users$account_language), decreasing = T))
languages
head(users$user_profile_description[users$account_language == "ar"], 10)
# Vermutlich ISO 639-1 bzw. lokalisierte (en-gb, zh-cn) Tags. Der Gro√üteil der Accounts sind englischsprachig eingestellt, aber ein knappes Drittel gibt russisch als Sprache an. West- u. Zentraleurop√§ische Accounts sind die drittgr√∂√üte Gruppe (Deutschland, UK, Frankreich, Spanien, Italien) vor arabischen Accounts. Vereinzelte Chinesen, Indonesier und Ukrainer.

#Aufbereitung als Data Frame, Gruppierung und Vorbereitung f√ºr Vergleich mit angegebenen Orten
lang1 <- as.character(languages[, 1])
lang2 <- languages[, 2]
other <- which(lang1 %in% c("zh-cn", "id"))
europe <- which(lang1 %in% c("de","en-gb", "fr", "es", "it"))
lang1 <- append(lang1, c("european", "others", "fantasy", "NA")) # fantasy + NA -> Platzhalter f√ºr sp√§ter
lang2 <- append(lang2, c(sum(lang2[europe]), sum(lang2[other]), 0, sum(is.na(users$account_language))))
lang1 <- lang1[-c(other, europe)]
lang2 <- lang2[-c(other, europe)]
loclang <- data.frame(lang1, lang2)
names(loclang) <- c("language", "language_n")
rm(lang1, lang2, other, europe)

loclang
sum(loclang$language_n)

### Orte
table(users$user_reported_location)
#Keine eindeutige Sortierung - wie zu vermuten bei individuellen Angaben - also manuelle Nachsortierung notwendig! (Im Folgenden hinter {Klammern} versteckbar f√ºr bessere √úbersicht und einfache Code-Ausf√ºhrung)
{
  users$shortened_location <- ifelse(users$user_reported_location %in% c("U.S.A", "USA", "Usa", "usa", "US", "us", "united states", "United States", "America", "AMERICA", "Amerika", "THE US", "mother America", "Murica", "Estados Unidos", "Douglas, United States"), "US", NA)
  # US-Bundesstaaten und D.C.
  users$shortened_location <- ifelse(users$user_reported_location %in% c("Troy, Alabama", "alabama", "AL"), "US, Alabama", users$shortened_location)
  users$shortened_location <- ifelse(users$user_reported_location %in% c("Alaska"), "US, Alaska", users$shortened_location)
  users$shortened_location <- ifelse(users$user_reported_location %in% c("Arizona, USA", "Arizona", "arizona", "ARIZONA", "City of Phoenix, Arizona", "Phoenix", "Mesa", "AZ"), "US, Arizona", users$shortened_location)
  users$shortened_location <- ifelse(users$user_reported_location %in% c("arkansas"), "US, Arkansas", users$shortened_location)
  users$shortened_location <- ifelse(users$user_reported_location %in% c("Venice, California", "Redwood City, California", "San-Francisco", "Oakland, CA", "California", "California, USA", "CA", "UC Davis", "Santa Barbara, California", "San Francisco", "San Francisco, CA","SF", "San Diego", "San DIego, CA", "San Diego, United States", "Rancho Rinconada, CA, USA", "San Diego, CA", "Riverside", "Santa Monica", "Los Gatos, California", "Los Angeles, CA", "Los ANgeles", "los angeles", "Los-Angeles", "Los Angeles", "Hollywood", "Fowler, California"), "US, California", users$shortened_location)
  users$shortened_location <- ifelse(users$user_reported_location %in% c("La Junta, Colorado", "Denver, CO", "Colorado"), "US, Colorado", users$shortened_location)
  users$shortened_location <- ifelse(users$user_reported_location %in% c("New Haven"), "US, Conneticut", users$shortened_location)
  users$shortened_location <- ifelse(users$user_reported_location %in% c("Delaware, USA"), "US, Delaware", users$shortened_location)
  users$shortened_location <- ifelse(users$user_reported_location %in% c("Weeki Wachee, Florida", "USA, FL", "North Port, FL", "Destin, Florida", "Orlando", "Orlando, FL", "Tallahassee", "Miami", "Miami, FL", "Miami, USA", "jacksonville", "Jacksonville", "Florida, USA", "Florida"), "US, Florida", users$shortened_location)
  users$shortened_location <- ifelse(users$user_reported_location %in% c("USA, Atlanta", "ATL", "ATL, GA", "Atlanta", "Atlanta, GA", "Atlanta, Georgia", "Macon, GA", "Georgia", "Georgia, USA", "Druid Hills, GA", "Downtown, Atlanta", "Brookhaven, GA"), "US, Georgia", users$shortened_location)
  users$shortened_location <- ifelse(users$user_reported_location %in% c(), "US, Hawaii", users$shortened_location)
  users$shortened_location <- ifelse(users$user_reported_location %in% c("ID"), "US, Idaho", users$shortened_location)
  users$shortened_location <- ifelse(users$user_reported_location %in% c("Aurora, Illinois", "Chicago", "Chicago, IL"), "US, Illinois", users$shortened_location)
  users$shortened_location <- ifelse(users$user_reported_location %in% c("INDIANAPOLIS, USA"), "US, Indiana", users$shortened_location)
  users$shortened_location <- ifelse(users$user_reported_location %in% c(), "US, Iowa", users$shortened_location)
  users$shortened_location <- ifelse(users$user_reported_location %in% c("Wichita, KS", "Kansas", "Kansas, USA"), "US, Kansas", users$shortened_location)
  users$shortened_location <- ifelse(users$user_reported_location %in% c("Louisville"), "US, Kentucky", users$shortened_location)
  users$shortened_location <- ifelse(users$user_reported_location %in% c("Baton Rouge, LA", "New Orleans, LA", "New Orleans", "New Orlean", "Louisiana", "LA", "Lafayette, LA"), "US, Louisiana", users$shortened_location)
  users$shortened_location <- ifelse(users$user_reported_location %in% c("Brunswick, ME, USA"), "US, Maine", users$shortened_location)
  users$shortened_location <- ifelse(users$user_reported_location %in% c("Baltimore", "Baltimore, MD", "temple hills, md"), "US, Maryland", users$shortened_location)
  users$shortened_location <- ifelse(users$user_reported_location %in% c("Salem, Massachusetts", "Boston", "Boston, MA", "Boston, USA"), "US, Massachusetts", users$shortened_location)
  users$shortened_location <- ifelse(users$user_reported_location %in% c("Bloomfield Hills, Michigan", "Michigan", "Flint, MI", "Detroit", "Detroit, Michigan"), "US, Michigan", users$shortened_location)
  users$shortened_location <- ifelse(users$user_reported_location %in% c("MN", "Mn", "Minnesota", "Minneapolis, MN", "Menisotta"), "US, Minnesota", users$shortened_location)
  users$shortened_location <- ifelse(users$user_reported_location %in% c("Jackson, MS", "Jackson"), "US, Mississippi", users$shortened_location)
  users$shortened_location <- ifelse(users$user_reported_location %in% c("StLouis", "St Louis, MO", "Missouri, USA", "Kansas City, MO", "Ferguson, MO"), "US, Missouri", users$shortened_location)
  users$shortened_location <- ifelse(users$user_reported_location %in% c("Montana"), "US, Montana", users$shortened_location)
  users$shortened_location <- ifelse(users$user_reported_location %in% c("Nebraska, USA"), "US, Nebraska", users$shortened_location)
  users$shortened_location <- ifelse(users$user_reported_location %in% c("Vegas", "Nevada", "Las Vegas"), "US, Nevada", users$shortened_location)
  users$shortened_location <- ifelse(users$user_reported_location %in% c("NH", "New Hampshire"), "US, New Hampshire", users$shortened_location)
  users$shortened_location <- ifelse(users$user_reported_location %in% c("NJ", "Newark, NJ", "Camden, NJ", "New Jersey", "New Jersey, USA", "Old Bridge, New Jersey"), "US, New Jersey", users$shortened_location)
  users$shortened_location <- ifelse(users$user_reported_location %in% c("NM", "Albuquerque", "Albuquerque, NM", "Eldorado at Santa Fe, NM, USA", "Los Alamos"), "US, New Mexico", users$shortened_location)
  users$shortened_location <- ifelse(users$user_reported_location %in% c("NYC", "nyc", "NY City", "NY", "ny", "New York, USA", "New York, NY", "New York City", "New - York", "New-York", "Bronx, NY", "brooklyn", "Brooklyn", "Brooklyn, NY", "Brkln", "The Big Apple", "Queens, NY", "New York", "Manhattan, NY", "Garden City, NY", "Johnson City, New York", "East Aurora, New York", "Baldwinsville, New York", "Buffalo"), "US, New York", users$shortened_location)
  users$shortened_location <- ifelse(users$user_reported_location %in% c("North Carolina", "Raleigh, North Carolina", "Greensboro"), "US, North Carolina", users$shortened_location)
  users$shortened_location <- ifelse(users$user_reported_location %in% c("North Dakota"), "US, North Dakota", users$shortened_location)
  users$shortened_location <- ifelse(users$user_reported_location %in% c("Whitehall, Ohio", "Cincinnati, OH", "Cleveland, OH", "cleveland / ohio", "Cleve", "Columbus, Ohio", "Ohio", "Ohio, USA", "OH", "Montgomery", "Montgomery, Ohio", "Millville village, OH, USA", "Milford, Ohio", "Grove City, Ohio", "Green village, OH, USA", "City of Cleveland, USA", "Cincinnati"), "US, Ohio", users$shortened_location)
  users$shortened_location <- ifelse(users$user_reported_location %in% c("Oklahoma", "Oklahoma City", "Oklahoma, PA", "Oklahoma, USA"), "US, Oklahoma", users$shortened_location)
  users$shortened_location <- ifelse(users$user_reported_location %in% c("portland", "Portland"), "US, Oregon", users$shortened_location)
  users$shortened_location <- ifelse(users$user_reported_location %in% c("Pittsburgh", "Pittsburgh, PA", "Pittsburgh, US", "Philadelhia", "Philadelphia", "Philadelphia, PA", "Philly", "Pennsylvania", "Mohnton, PA", "Chester, PA"), "US, Pennsylvania", users$shortened_location)
  users$shortened_location <- ifelse(users$user_reported_location %in% c("Rhode Island", "Rhode island"), "US, Rhode Island", users$shortened_location)
  users$shortened_location <- ifelse(users$user_reported_location %in% c("South Carolina, USA", "Columbia, SC", "Charleston, SC"), "US, South Carolina", users$shortened_location)
  users$shortened_location <- ifelse(users$user_reported_location %in% c(), "US, South Dakota", users$shortened_location)
  users$shortened_location <- ifelse(users$user_reported_location %in% c("Tennessee, USA", "TN", "Nashville", "Memphis", "Memphis, TN"), "US, Tennessee", users$shortened_location)
  users$shortened_location <- ifelse(users$user_reported_location %in% c("Texas, USA", "Texas", "Austin", "Austin, TX", "Ostin", "All Over Texas", "City of San Antonio, TX", "Dallas", "Dallas, Texas", "Stonewall, TX", "Houston", "Houston, TX", "El Paso, Texas", "Dallas, TX"), "US, Texas", users$shortened_location)
  users$shortened_location <- ifelse(users$user_reported_location %in% c("Utah", "utah", "Salt Lake City"), "US, Utah", users$shortened_location)
  users$shortened_location <- ifelse(users$user_reported_location %in% c(), "US, Vermont", users$shortened_location)
  users$shortened_location <- ifelse(users$user_reported_location %in% c("Winchester, Virginia", "Richmond, VA"), "US, Virginia", users$shortened_location)
  users$shortened_location <- ifelse(users$user_reported_location %in% c("WA", "Washington", "Seattle, WA", "Seattle", "Stanwood city, WA, USA"), "US, Washington", users$shortened_location)
  users$shortened_location <- ifelse(users$user_reported_location %in% c(), "US, West Virginia", users$shortened_location)
  users$shortened_location <- ifelse(users$user_reported_location %in% c("Wisconsin, USA", "Milwaukee", "Milwaukee, WI", "Madison"), "US, Wisconsin", users$shortened_location)
  users$shortened_location <- ifelse(users$user_reported_location %in% c(), "US, Wyoming", users$shortened_location)
  users$shortened_location <- ifelse(users$user_reported_location %in% c("Washington D.C", "Washington D.C.", "Washington, D.C.", "Washington, DC"), "US, Washington D.C.", users$shortened_location)
  users$shortened_location <- ifelse(users$user_reported_location %in% c("Guanica zona urbana, PR, USA"), "US, Unincorporated", users$shortened_location)
  # Russland
  users$shortened_location <- ifelse(users$user_reported_location %in% c("Krasnoyarsk", "Krasnoyarsk, Russia", "Ekaterinburg, Russia", "Chelyabinsk, Russia", "Volgograd", "Velikiy Novgorod, Russia", "Stavropol, Russia", "Ufa, Russia", "Rostov-na-Donu, Russia", "russia", "Russia", "Russian Empire", "novgorod", "Tomsk", "Republic of Chechnya, Russia", "Crimea, Russia", "Perm, Russia", "Penza", "Omsk, Russia", "Nizhniy Novgorod, Russia", "Murmansk, Russia", "Irkutsk", "Chelyaba", "belgorod", "‚ú¥–ù–æ–≤–≥–æ—Ä–æ–¥‚ú¥", "–£—Ñ–∞", "–£–ª—å—è–Ω–æ–≤—Å–∫", "–¢—É–ª–∞, –¢—É–ª—å—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å", "—É—Ñ–∞", "—É–ª. –õ–µ–Ω–∏–Ω–∞", "—Ä–æ—Å—Ç–æ–≤-—è-—Ç–æ–Ω—É", "—Ä–æ—Å—Å–∏—è", "–æ–º—Å–∫", "–Ω–æ–≤–æ—Å–∏–±–∏—Ä—Å–∫", "–Ω–æ–≤–≥–æ—Ä–æ–¥", "–µ–∫–±", "–¥–Ω–∏—â–Ω–∏–π –¥–Ω–æ–≤–≥–æ—Ä–æ–¥", "–Ø—Ä–æ—Å–ª–∞–≤–ª—å, –†–æ—Å—Å–∏—è", "–Ø—Ä–æ—Å–ª–∞–≤–ª—å", "–ß–∏—Ç–∞", "–ß–µ—á–µ–Ω—Å–∫–∞—è —Ä–µ—Å–ø—É–±–ª–∏–∫–∞, –†–æ—Å—Å–∏—è", "–ß–µ–ª—è–±–∏–Ω—Å–∫", "–ß–µ–ª–Ω—ã", "–ß–µ–±–æ–∫—Å–∞—Ä—ã, –†–æ—Å—Å–∏—è", "–ß–µ–±–æ–∫—Å–∞—Ä—ã", "–•–∞–Ω—Ç—ã-–ú–∞–Ω—Å–∏–π—Å–∫", "–•–∞–±–∞—Ä–æ–≤—Å–∫, –†–æ—Å—Å–∏—è", "–•–∞–±–∞—Ä–æ–≤—Å–∫", "–¢—É–ª–∞, –†–æ—Å—Å–∏—è", "–¢—É–ª–∞", "–¢—É–≤–∞", "–¢–æ–º—Å–∫", "–¢–æ–ª—å—è—Ç—Ç–∏", "–¢–≤–µ—Ä—å", "–¢–≤–µ—Ä—Å–∫–æ–µ –ø–æ–¥–≤–æ—Ä—å–µ", "–¢–∞—à–ª–∞", "–¢–∞–º–∞–Ω—å", "–°—Ä–∞—Ä–∞—Ç–æ–≤", "–°–æ—á–∏, –†–æ—Å—Å–∏—è", "–°–æ—á–∏", "–°–º–æ–ª–µ–Ω—Å–∫", "–°–∏–º—Ñ–µ—Ä–æ–ø–æ–ª—å", "–°–∞—Ä–æ–≤", "–°–∞—Ä–∞—Ç–æ–≤ )))", "–°–∞—Ä–∞—Ç–æ–≤", "–°–∞—Ä–∞–Ω—Å–∫", "–°–∞–º–∞—Ä–∞", "–†—è–∑–∞–Ω—å", "–†–æ—Å—Ç–æ–≤-–Ω–∞-–î–æ–Ω—É, –†–æ—Å—Å–∏—è", "–†–æ—Å—Ç–æ–≤-–Ω–∞-–î–æ–Ω—É", "–†–æ—Å—Ç–æ–≤-–ù–∞-–î–æ–Ω—É"), "Russia", users$shortened_location)
  users$shortened_location <- ifelse(users$user_reported_location %in% c("–†–æ—Å—Ç–æ–≤", "–†–æ—Å—Å–∏—è, –ö–∞–∑–∞–Ω—å", "–†–æ—Å—Å–∏—è", "–†–æ—Å—Å–∏–π—Å–∫–∞—è –§–µ–¥–µ—Ä–∞—Ü–∏—è", "–†–§", "–ü—è—Ç–∏–≥–æ—Ä—Å–∫", "–ü–µ–Ω–∑–∞", "–û—Ä–µ–ª", "–û–º—Å–∫, –†–æ—Å—Å–∏—è", "–û–º—Å–∫", "–ù—è–≥–∞–Ω—å", "–ú—É—Ä–º–∞–Ω—Å–∫", "–ú–∞–≥–∞–¥–∞–Ω", "–õ—é–±–µ—Ä—Ü—ã", "–õ—É–≥–∞", "–õ–∏–ø–µ—Ü–∫", "–ö—É—Ä—Å–∫", "–ö—Ä–∞—Å–Ω–æ—è—Ä—Å–∫", "–ö–∏—Ä–æ–≤", "–ö–µ–º–µ—Ä–æ–≤–æ", "–ü–µ—Ç—Ä–æ–∑–∞–≤–æ–¥—Å–∫", "–ü–µ—Ä–º—å", "–æ–≤–æ—Å–∏–±–∏—Ä—Å–∫, –†–æ—Å—Å–∏—è", "–ù–æ–≤–æ—Å–∏–±–∏—Ä—Å–∫", "–ù–æ–≤–æ—Å–∏–±", "–ù–æ–≤–æ–∫—É–∑–Ω–µ—Ü–∫, –†–æ—Å—Å–∏—è", "–ù–æ–≤–≥–æ—Ä–æ–¥", "–ù–æ–≤–æ—Å–∏–±–∏—Ä—Å–∫, –†–æ—Å—Å–∏—è", "–ö—Ä–∞—Å–Ω–æ–¥–∞—Ä", "–ö–æ—Ç–ª–∞—Å", "–ö–æ—Å—Ç—Ä–æ–º–∞", "–ö–æ–ª—å—Å–∫–∏–π –ø-–≤", "–ù–æ–≤–æ—Å–∏–±–∏—Ä—Å–∫, –†–æ—Å—Å–∏—è", "–ù–∏–∂–Ω–∏–π –ù–æ–≤–æ–≥–æ—Ä–æ–¥", "–ù–∏–∂–Ω–∏–π –ù–æ–≤–≥–æ—Ä–æ–¥", "–ù–∏–∂–Ω–∏–π", "–ï–∫–∞—Ç–µ—Ä–∏–Ω–±—É—Ä–≥...", "–ï–∫–∞—Ç–µ—Ä–∏–Ω–±—É—Ä–≥", "–ï–ö–ë", "–Å-–±—É—Ä–≥", "–ù–∏–∂–Ω–∏–π –ù–æ–≤–≥–æ—Ä–æ–¥", "–ù–∏–∂–Ω–∏–π", "–ù–∞–±–µ—Ä–µ–∂–Ω—ã–µ –ß–µ–ª–Ω—ã, –†–æ—Å—Å–∏—è", "–ö–∞–ª—É–≥–∞, –†–æ—Å—Å–∏—è", "–ö–∞–ª—É–≥–∞", "–ö–∞–∑–∞–Ω—å", "–ö–∞–ª–∏–Ω–∏–Ω–≥—Ä–∞–¥, –†–æ—Å—Å–∏—è", "–ö–∞–ª–∏–Ω–∏–Ω–≥—Ä–∞–¥", "–ò—Ä–∫—É—Ç—Å–∫", "–ò–≤–∞–Ω–æ–≤–æ", "–ï–ª–µ—Ü", "–ë–∞—Ä–Ω–∞—É–ª, –†–æ—Å—Å–∏—è"), "Russia", users$shortened_location)
  users$shortened_location <- ifelse(users$user_reported_location %in% c( "–ì—Ä–æ–∑–Ω—ã–π", "–ì—Ä–æ–∑–Ω—ã–π", "–í—Å—è –†–æ—Å—Å–∏—è", "–í—Å—è –†–æ—Å—Å–∏—è", "–í–æ—Ä–æ–Ω–µ–∂, –†–æ—Å—Å–∏—è", "–í–æ—Ä–æ–Ω–µ–∂", "–í–æ–ª–æ–≥–¥–∞", "–í–æ–ª–≥–æ–≥—Ä–∞–¥, –†–æ—Å—Å–∏—è", "–í–æ–ª–≥–æ–≥—Ä–∞–¥, –í–∞—Å—è!", "–í–æ–ª–≥–æ–≥—Ä–∞–¥", "–í–æ–ª–≥–æ–¥–æ–Ω—Å–∫, –†–æ—Å—Å–∏—è", "–í–µ–ª–∏–∫–∏–π –ù–æ–≤–≥–æ—Ä–æ–¥", "–ë—Ä—è–Ω—Å–∫", "–ë–µ–ª–≥–æ—Ä–æ–¥", "–ë–∞—à–∫–∞—Ä—Ç–æ—Å—Ç–∞–Ω", "–ê—Ä—Ö–∞–Ω–≥–µ–ª—å—Å–∫, –†–æ—Å—Å–∏—è", "–ê—Ä—Ö–∞–Ω–≥–µ–ª—å—Å–∫"), "Russia", users$shortened_location)
  # Aufsplittung in mehrere Befehle umgeht komische Sytax-Fehler
  users$shortened_location <- ifelse(users$user_reported_location %in% c("Russia, Moscow", "moscow city", "Moscow-City", "Moscow", "msk", "Msk", "MSK", "MSK SAO", "‚òÖ–º–æ—Å–∫–≤–∞", "Moscow, Russia", "moscow", "–ú–æ—Å–∫–≤–∞‚ô•", "–ú–æ—Å–∫–æ—É-—Å–∏—Ç–∏", "–æ—Å–∫–≤–∞, –†–æ—Å—Å–∏—è", "–ú–æ—Å–∫–≤–∞ –ó–ª–∞—Ç–æ–≥–ª–∞–≤–∞—è", "–ú–æ—Å–∫–≤–∞ –ë–µ–ª–æ–∫–∞–º–µ–Ω–Ω–∞—è", "–ú–æ—Å–∫–≤–∞ (–°–°–°–† - –†–æ—Å—Å–∏—è)", "–ú–æ—Å–∫–≤–∞ - —Å—Ç–æ–ª–∏—Ü–∞", "–ú–æ—Å–∫–≤–∞ - –°–∞–º–∞—Ä–∞", "–ú–æ—Å–∫–≤–∞ - –õ–æ–Ω–¥–æ–Ω", "–ú–æ—Å–∫–≤–∞", "–ú–æ—Å–∫–∞", "–º—Å–∫", "–º–æ—Å–∫–≤–∞", "–¢—É–∞–ø—Å–µ, –ú–æ—Å–∫–≤–∞", "–°–µ—Ä–ø—É—Ö–æ–≤", "–°–µ–≤–∞—Å—Ç–æ–ø–æ–ª—å", "–†–æ—Å—Å–∏—è, –ú–æ—Å–∫–≤–∞", "–ü—É—à–∫–∏–Ω–æ", "–ü—Å–∫–æ–≤", "–ü–æ–¥–æ–ª—å—Å–∫", "–û–¥–∏–Ω—Ü–æ–≤–æ", "–ú–æ—Å–∫–≤–∞, –†–æ—Å—Å–∏—è", "–ú–°–ö–í–ê", "–ú–°–ö", "–ö–æ–ª–æ–º–Ω–∞", "–ü–µ—Ä–æ–≤–æ", "–úSK", "–î–º–∏—Ç—Ä–æ–≤"), "Russia, Moscow", users$shortened_location) # Moskau-Stadt + Oblast
  users$shortened_location <- ifelse(users$user_reported_location %in% c("St-Petersburg", "St. Petersburg", "st.petersburg", "St.Petersburg", "st/petersburg", "Saint Petersburg, Russia", "Saint Petersburg", "saint-petersburg", "Saint-Peterburg", "saint P.", "saint p.", "Saint-P.", "Saint-P", "St.P", "St.P.", "St-P", "SPb‚ô•‚óã‚Ä¢¬∞‚Ä¢‚òÜ", "SPB", "SPb", "spb", "c–∞–Ω–∫—Ç-–ø–µ—Ç–µ—Ä–±—É—Ä–≥", "—Å–ø–±", "–°–ø–±", "–°–µ—Å—Ç—Ä–æ—Ä–µ—Ü–∫, –†–æ—Å—Å–∏—è", "–°–∞–Ω–∫—Ç-–ø–µ—Ç–µ—Ä–±—É—Ä–≥", "–°–∞–Ω–∫—Ç-–ü–µ—Ç–µ—Ä–±—É—Ä–≥, –†–æ—Å—Å–∏—è", "–†–æ—Å—Å–∏—è –°–∞–Ω–∫—Ç-–ü–µ—Ç–µ—Ä–±—É—Ä–≥", "–°–∞–Ω–∫—Ç-–ü–µ—Ç–µ—Ä–±—É—Ä–≥", "–°–ü–±", "–°–ü–ë–ì–ü–£", "–°–ü–ë", "–°.–ü–µ—Ç–µ—Ä–±—É—Ä–≥", "–°-–ü–±", "–ü—É—à–∫–∏–Ω, –°–∞–Ω–∫—Ç-–ü–µ—Ç–µ—Ä–±—É—Ä–≥", "–ü—É—à–∫–∏–Ω", "–ü–ï–¢–ï–†–ë–£–†–ì", "–õ–æ—Å-–ü–∏—Ç–µ—Ä–±—É—Ä–≥–æ—Å", "–õ–µ–Ω–∏–Ω–≥—Ä–∞–¥", "–ö—Ä–æ–Ω—à—Ç–∞–¥—Ç", "–ö–∏–Ω–≥–∏—Å–µ–ø–ø", "–ü–µ—Ç—Ä–æ–≥—Ä–∞–¥", "–ü–µ—Ç–µ—Ä–±—É—Ä–≥", "–í—ã–±–æ—Ä–≥"), "Russia, St.Petersburg", users$shortened_location) # St.Petersburg + Oblast Leningrad
  # Sonstige L√§nder
  users$shortened_location <- ifelse(users$user_reported_location %in% c("Kiel, Schleswig-Holstein", "K√∂ln, Deutschland.", "Hessen, Deutschland", "Hamburg, Deutschland", "Frankfurt am Main, Deutschland", "Frankfurt am Main, Hessen", "Erfurt, Deutschland", "D√ºsseldorf, Deutschland", "Dresden, Sachsen", "Bremen, Deutschland", "Berlin, Deutschland", "Stuttgart, Deutschland", "Rostock, Deutschland", "Saarbr√ºcken, Deutschland", "Deutschland", "M√ºnchen, Bayern", "Magdeburg, Deutschland", "K√∂ln, Deutschland", "–ü–æ—Ç—Å–¥–∞–º"), "Germany", users$shortened_location)
  users$shortened_location <- ifelse(users$user_reported_location %in% c("UK", "Newcastle", "Newport", "Manchester", "London", "London, England", "London, UK", "Liverpool", "Coventry"), "United Kingdom", users$shortened_location)
  users$shortened_location <- ifelse(users$user_reported_location %in% c("Italy", "Italia", "italia", "Milano, Lombardia", "Itala, Sicilia"), "Italy", users$shortened_location)
  users$shortened_location <- ifelse(users$user_reported_location %in% c("Paris", "Paris, France", "Lyon"), "France", users$shortened_location)
  users$shortened_location <- ifelse(users$user_reported_location %in% c("Brussel, Belgi√´"), "Belgium", users$shortened_location)
  users$shortened_location <- ifelse(users$user_reported_location %in% c("–°—Ç–æ–∫–≥–æ–ª—å–º"), "Sweden", users$shortened_location)
  users$shortened_location <- ifelse(users$user_reported_location %in% c("–£–∫—Ä–∞–∏–Ω–∞", "–ü–æ–∫—Ä–æ–≤—Å–∫–æ–µ", "–û–¥–µ—Å—Å–∞", "–ú–∞—Ä–∏—É–ø–æ–ª—å", "–õ—É—Ü–∫", "–õ—É–≥–∞–Ω—Å–∫", "–ö—Ä–µ–º–µ–Ω—á—É–≥", "–ö–∏—ó–≤", "–ö–∏–µ–≤", "–ó–∞–ø–æ—Ä–æ–∂—å–µ", "–ñ–∏—Ç–æ–º–∏—Ä", "–î–æ–Ω–µ—Ü–∫, –†–æ—Å—Å–∏—è", "–î–æ–Ω–µ—Ü–∫", "–î–Ω–µ–ø—Ä, –£–∫—Ä–∞–∏–Ω–∞", "–î–Ω–µ–ø—Ä"), "Ukraine", users$shortened_location) # Krim zu Russland, Donetsk zu Ukraine
  users$shortened_location <- ifelse(users$user_reported_location %in% c("–¢–∞–ª–ª–∏–Ω"), "Estonia", users$shortened_location)
  users$shortened_location <- ifelse(users$user_reported_location %in% c("–ü—Ä–∞–≥–∞"), "Czechia", users$shortened_location)
  users$shortened_location <- ifelse(users$user_reported_location %in% c("–ú—ñ–Ω—Å–∫", "–í–∏—Ç–µ–±—Å–∫", "–ë–µ–ª–∞—Ä—É—Å—å"), "Belarus", users$shortened_location)
  users$shortened_location <- ifelse(users$user_reported_location %in% c("–°—Ç–∞–º–±—É–ª"), "Turkey", users$shortened_location)
  users$shortened_location <- ifelse(users$user_reported_location %in% c("Cairo, Egypt", "ŸÖÿµÿ±"), "Egypt", users$shortened_location)
  users$shortened_location <- ifelse(users$user_reported_location %in% c("Beirut", "ŸÑÿ®ŸÜÿßŸÜ", "ÿ®Ÿäÿ±Ÿàÿ™"), "Lebanon", users$shortened_location)
  users$shortened_location <- ifelse(users$user_reported_location %in% c("ÿßŸÑÿπÿ±ÿßŸÇ"), "Iraq", users$shortened_location)
  users$shortened_location <- ifelse(users$user_reported_location %in% c("–±–∞–∫—É"), "Azerbaijan", users$shortened_location)
  users$shortened_location <- ifelse(users$user_reported_location %in% c("syria", "Syria", "Damascus", "Aleppo", "ÿ≥Ÿàÿ±Ÿäÿ©", "ÿØŸÖÿ¥ŸÇ", "ÿ≥Ÿàÿ±Ÿäÿß", "ÿ≠ŸÖÿßÿ©", "ÿ≠ŸÖÿµ"), "Syria", users$shortened_location)
  users$shortened_location <- ifelse(users$user_reported_location %in% c("Oliva Pizza & Pasta // Amman", "Zarqa, Jordan", "Amman", "Az-Zarqa", "ÿ≠ŸÑÿ®", "ÿßŸÑŸÑÿßÿ∞ŸÇŸäÿ©", "ÿßŸÑŸÖŸÖŸÑŸÉÿ© ÿßŸÑÿ£ÿ±ÿØŸÜŸäÿ© ÿßŸÑŸáÿßÿ¥ŸÖŸäÿ©", "ÿßŸÑÿ£ÿ±ÿØŸÜ"), "Jordan", users$shortened_location)
  users$shortened_location <- ifelse(users$user_reported_location %in% c("ÿßŸÑÿØŸÖÿßŸÖ"), "Saudi-Arabia", users$shortened_location)
  users$shortened_location <- ifelse(users$user_reported_location %in% c("–ú–∞–Ω–∏–ª–∞"), "Philippines", users$shortened_location)
  users$shortened_location <- ifelse(users$user_reported_location %in% c("Manama, Bahrain"), "Bahrain", users$shortened_location)
  users$shortened_location <- ifelse(users$user_reported_location %in% c("ÿπŸÖÿßŸÜ"), "Oman", users$shortened_location)
  users$shortened_location <- ifelse(users$user_reported_location %in% c("–ü—Ö–µ–Ω—å—è–Ω"), "North Korea", users$shortened_location)
  users$shortened_location <- ifelse(users$user_reported_location %in% c("katamandu"), "Nepal", users$shortened_location)
  users$shortened_location <- ifelse(users$user_reported_location %in% c("ZM"), "Zambia", users$shortened_location)
  # Other
  users$shortened_location <- ifelse(users$user_reported_location %in% c("Universe", "Wonderland", "sin city", "Love Town", "Liberty City", "Fattyland", "garage", "hood", "I AM A CITIZEN OF THE UNIVERSE", "Islamic States of America", "dreamland", "Black America", "—Ö–æ–≥–≤–æ—Ä—Ç—Å", "Cosmopolitanism", "2148", "ÿßŸÑÿØŸàŸÑÿ© ÿßŸÑÿ•ÿ≥ŸÑÿßŸÖŸäÿ©", "–º–æ–π –º–∏—Ä", "–∞–º—Ñ–µ—Ç–∞–º–∏–Ω–æ–≤–∞—è —Å—Ç–æ–ª–∏—Ü–∞", "–Ø –≤–µ–∑–¥–µ", "–ß–µ—Ä–µ–º—É—Ö–∏", "–¶–ê–û", "–¢—Ä–µ—Ç–∏–π –†–∏–º",  "–†–∞—à–∞", "–†–∞–±–æ—Ç–∞))", "–ú–µ—á—Ç–æ–≤–∏–ª—å", "–ú–µ—á—Ç–∞", "–ö—Ä–∞—Å—Ç–∏ –ë—É—Ä–≥–µ—Ä", "–ê—Ä—Ç–µ–º", "#–†—É—Å—Å–∫–∏–π–ú–∏—Ä"), "Other", users$shortened_location)
  users$shortened_location <- ifelse(users$user_reported_location %in% c("Newhustle", "vladik", "Man", "piter", "Jersey", "KN", "Birmingham", "Cromwell", "—à–∏—Ä–∫–∏–Ω–æ", "Ch", "—Å–∂–∏–≥–∞–π —Ç–æ–ª–µ—Ä–∞–Ω—Ç–Ω—ã–π —Ä–∞–π,—Å–ª—ã—à–∏—à—å", "—Å–µ–π—á–∞—Å –æ—Ä–µ–ª", "–¢–∞–ª–ª–∏–Ω, –°–∞–Ω–∫—Ç-–ü–µ—Ç–µ—Ä–±—É—Ä–≥", "–°–∏–±–∏—Ä—å –º–∞—Ç—É—à–∫–∞", "–°–µ–≤–µ—Ä–Ω–∞—è —Å—Ç–æ–ª–∏—Ü–∞", "‚ô†–ü–∏—Ç–µ—Ä‚ô†", "–ø–∏—Ç–µ—Ä", "–ü–∏—Ç–µ—Ä", "–ù–æ–≤–≥–æ—Ä–æ–¥ - –°–ü–±", "–ù–ù", "–í–ª–∞–¥–∏–∫ –î–í–§–£", "–í–∏–ª—å–Ω—é—Å - –ú–æ—Å–∫–≤–∞", "–í–∞—Ç–Ω–æ–≥—Ä–∞–¥", "–ë—É—Ç–æ–≤–æ", "–ê–¥–ª–µ—Ä", "Adler", "‚òº–ù–æ–≤–æ—Å–∏–±—á–∏–∫‚òº"), "Unidentified", users$shortened_location)
}
table(users$shortened_location)
sum(is.na(users$shortened_location))
length(grep("US", users$shortened_location))
length(grep("Russia", users$shortened_location))

#Grouping
arabic <- length(grep("Bahrain", users$shortened_location)) + length(grep("Egypt", users$shortened_location)) + length(grep("Lebanon", users$shortened_location)) + length(grep("Iraq", users$shortened_location)) + length(grep("Syria", users$shortened_location)) + length(grep("Jordan", users$shortened_location)) + length(grep("Saudi-Arabia", users$shortened_location)) + length(grep("Oman", users$shortened_location))
easteuro <- length(grep("Belarus", users$shortened_location)) + length(grep("Czechia", users$shortened_location)) + length(grep("Estonia", users$shortened_location)) + length(grep("Ukraine", users$shortened_location)) + length(grep("United Kingdom", users$shortened_location))
westeuro <- length(grep("Belgium", users$shortened_location)) + length(grep("France", users$shortened_location)) + length(grep("Germany", users$shortened_location)) + length(grep("Italy", users$shortened_location)) + length(grep("Sweden", users$shortened_location))
other <- length(grep("Azerbaijan", users$shortened_location)) + length(grep("Nepal", users$shortened_location)) + length(grep("North Korea", users$shortened_location)) + length(grep("Philippines", users$shortened_location)) + length(grep("Turkey", users$shortened_location)) + length(grep("Zambia", users$shortened_location))

loc1 <- c("US", "Russia", "Arabic", "East Europe", "West Europe", "other", "Fantasy", "NA")
loc2 <- c(length(grep("US", users$shortened_location)), length(grep("Russia", users$shortened_location)), arabic, easteuro, westeuro, other, (length(grep("Unidentified", users$shortened_location)) + length(grep("Fantasy", users$shortened_location))), sum(is.na(users$shortened_location)))
loclang <- loclang %>% mutate(location = loc1, location_n = loc2)
rm(arabic, easteuro, westeuro, other, loc1, loc2)
loclang
# Konsistenz zwischen den Spracheinstellungen und Ortsangaben. Englisch (en) √ºberwiegt bei den Sprachen zwar deutlich im Vergleich mit den angegebene Orten, aber da englisch global dominant ist, ist davon auszugehen, dass auch Nutzer in anderen L√§ndern ihre Accounts auf englisch einstellen (-> Europa) - oder dass es einfach die Standardeinstellung ist, und diese Nutzer sie nie ge√§ndert haben. Der deutliche Anstieg in Osteuropa l√§sst sich durch die Tatsache erkl√§ren, dass bis auf zwei ukrainisch-sprachige Angaben (was sich mit den languages deckt) alle Ortsangaben aus diesem Gebiet auf russisch waren.


### Account-Erstelldaten
quartals <- c(as.Date("2009-01-01"), as.Date("2009-04-01"), as.Date("2009-07-01"), as.Date("2009-10-01"))
for(i in 10:18){
  year <- as.character(2000 + i)
  for(q in c("-01-01", "-04-01", "-07-01", "-10-01")){
    quartals <- append(quartals, as.Date(paste(year, q, sep ="")))
  }
}
rm(i, q, year)
quartals <-  quartals[1:39]
hist(users$account_creation_date, breaks = quartals)
# Der Gro√üteil der Accounts wurde im Zeitraum 2. H√§fte 2013 - 1. H√§fte 2014 erstellt - lange vor dem US-Wahlkampf 2016. M√∂gliche Erkl√§rungen: Zeitnutzung, um Accounts als "seri√∂s" zu etablieren, oder Nutzung der Accounts zur Beeinflussung anderer Themen als der Wahl.


### Gefolgte Accounts
hist(users$following_count)
hist(log2(users$following_count))
summary(users$following_count)
# Ein Gro√üteil der Accounts folgt nur ein paar Hundert Accounts (wenn √ºberhaupt), w√§hrend einige wenige Accounts mehreren zehntausend Accounts folgen -> Wahrscheinlich Nutzung von Follow-Bots, um Nummern zu erh√∂hen.

followbots <- users[which(users$following_count >= 10000), ]
followbots <- followbots[, 7:8]
followbots
summary(lm(followbots$follower_count ~ followbots$following_count))
summary(lm(users$follower_count ~ users$following_count))
# F√ºr die Accounts, die √ºber 10.000 anderen Accounts folgen, besteht ein deutlicher Zusammenhang zwischen der Anzahl gefolgter Accounts und der Anzahl eigener Follower. W√§hrend f√ºr alle 3.600 Accounts die Anzahl gefolgter Accounts gut 25% der Varianz in den eigenen Follower-Zahlen erkl√§rt (adj. R^2 = 0,246), ist es f√ºr die Accounts mit √ºber 10.000 Follows eine Varianzaufkl√§rung von √ºber 60% (adj. R^2 = 0,611)!



# Analyse Tweets ----

### Tweet-Zeiten
times <- tibble(dt = tweets$tweet_time %>% ymd_hms()) %>%
  mutate(timepart = hms::hms(as.numeric(dt - floor_date(dt, "1 day"), unit="secs")), timeset = as.integer(substr(timepart,1,2)))
ggplot(times, aes(x = timeset)) + geom_bar() + theme_minimal() + ggtitle("Tweets by UTC time, all tweets")
#Es gibt deutlich erkennbare Muster in den Tweetzeiten. So werden eine gro√üe Menge Tweets zwischen 07:00 und 17:00 UTC abgesetzt, w√§hrend zwischen 21:00 und 06:00 UTC bedeutend weniger Tweets verfasst wurden.
#Das bedeutet, dass die Gro√üzahl der Tweets nach amerikanischer Sicht zwischen 03:00 und 13:00 (Ostk√ºste) bzw. 00:00 und 10:00 (Westk√ºste) verfasst wurden. Geht man von russischen Verfassern aus, so liegt die Hochfrequenz zwischen 10:00 und 20:00 (Moskau/St. Petersburg). Es gibt also entweder Hochzeiten w√§hrend der Morgensunden in Amerika oder w√§hrend den Arbeitsstunden in West-Russland.

times_eng <- tweets %>% filter(tweet_language %in% c("en")) %>% tibble(dt = tweet_time %>% ymd_hms()) %>%
  mutate(timepart = hms::hms(as.numeric(dt - floor_date(dt, "1 day"), unit="secs")), timeset = as.integer(substr(timepart,1,2)))
ggplot(times_eng, aes(x = timeset)) + geom_bar() + theme_minimal() + ggtitle("Tweets by UTC time, english language tweets")
# Filtert man nur nach englischsprachigen Tweets, so verschiebt sich das Maximum auf 13:00-17:00 UTC, mit einem Minimum zwischen 03:00-08:00 UTC.
#Somit sind die Minima bei 20:00-01:00 (Westk√ºste) und 23:00-04:00 (Ostk√ºste), bzw. 6:00-11:00 (Moskau) und die Maxima bei 06:00-01:00 (Westk√ºste) und 09:00-04:00 (Ostk√ºste), bzw. 16:00-11:00 (Moskau).


### Tweet-Sprachen nach Account

# Analyse der Anzahl englisch-/russischsprachiger Tweets f√ºr alle Accounts in den Daten, um nach sprachlicher EInheitlichkeit oder systematischen Ver√§nderungen zu suchen. Aufgrund der gro√üen Account-Anzahl (3608) aufgesplittet in mehrere Plots. Sollte sich im Plot-Fenster nach Ausf√ºhren des Print-Befehls kein Ergebnis zeigen, so kann es helfen, dieses zu vergr√∂√üern. Eine tats√§chliche Analyse der Grafiken ist in RStudio selbst nicht m√∂glich, die Dateien k√∂nnen jedoch als PDF exportiert und dann betrachtet werden - Exportma√üe von ca. 40x80‚Ä≥ werden empfohlen.
langplot_1 <- tweets %>% select(c(userid, tweet_language, tweet_time)) %>% filter(userid %in% users$userid[1:600]) %>%
  mutate(tweet_language =  ifelse(tweet_language == "en" | tweet_language == "ru", tweet_language, "other")) %>%
  ggplot(aes(x = as.Date(tweet_time), fill = tweet_language)) + geom_histogram() +
  scale_fill_manual(values = c("en" = "red", "ru" = "blue", "other" = "green")) + 
  theme(legend.position = "none") + facet_wrap(~ userid, ncol = 30, scales = "free_y")
langplot_2 <- tweets %>% select(c(userid, tweet_language, tweet_time)) %>% filter(userid %in% users$userid[601:1200]) %>%
  mutate(tweet_language =  ifelse(tweet_language == "en" | tweet_language == "ru", tweet_language, "other")) %>%
  ggplot(aes(x = as.Date(tweet_time), fill = tweet_language)) + geom_histogram() +
  scale_fill_manual(values = c("en" = "red", "ru" = "blue", "other" = "green")) + 
  theme(legend.position = "none") + facet_wrap(~ userid, ncol = 30, scales = "free_y")
langplot_3 <- tweets %>% select(c(userid, tweet_language, tweet_time)) %>% filter(userid %in% users$userid[1201:1800]) %>%
  mutate(tweet_language =  ifelse(tweet_language == "en" | tweet_language == "ru", tweet_language, "other")) %>%
  ggplot(aes(x = as.Date(tweet_time), fill = tweet_language)) + geom_histogram() +
  scale_fill_manual(values = c("en" = "red", "ru" = "blue", "other" = "green")) + 
  theme(legend.position = "none") + facet_wrap(~ userid, ncol = 30, scales = "free_y")
langplot_4 <- tweets %>% select(c(userid, tweet_language, tweet_time)) %>% filter(userid %in% users$userid[1801:2400]) %>%
  mutate(tweet_language =  ifelse(tweet_language == "en" | tweet_language == "ru", tweet_language, "other")) %>%
  ggplot(aes(x = as.Date(tweet_time), fill = tweet_language)) + geom_histogram() +
  scale_fill_manual(values = c("en" = "red", "ru" = "blue", "other" = "green")) + 
  theme(legend.position = "none") + facet_wrap(~ userid, ncol = 30, scales = "free_y")
langplot_5 <- tweets %>% select(c(userid, tweet_language, tweet_time)) %>% filter(userid %in% users$userid[2401:3000]) %>%
  mutate(tweet_language =  ifelse(tweet_language == "en" | tweet_language == "ru", tweet_language, "other")) %>%
  ggplot(aes(x = as.Date(tweet_time), fill = tweet_language)) + geom_histogram() +
  scale_fill_manual(values = c("en" = "red", "ru" = "blue", "other" = "green")) + 
  theme(legend.position = "none") + facet_wrap(~ userid, ncol = 30, scales = "free_y")
langplot_6 <- tweets %>% select(c(userid, tweet_language, tweet_time)) %>% filter(userid %in% users$userid[3001:3608]) %>%
  mutate(tweet_language =  ifelse(tweet_language == "en" | tweet_language == "ru", tweet_language, "other")) %>%
  ggplot(aes(x = as.Date(tweet_time), fill = tweet_language)) + geom_histogram() +
  scale_fill_manual(values = c("en" = "red", "ru" = "blue", "other" = "green")) + 
  theme(legend.position = "none") + facet_wrap(~ userid, ncol = 30, scales = "free_y")
# save("Saved Files/user-languages_ggplot.RData")
# DATEIEN LADEN: load("Saved Files/user-languages_ggplot.RData")


print(langplot_1)
print(langplot_2)
print(langplot_3)
print(langplot_4)
print(langplot_5)
print(langplot_6)
# Relative Einheitlichkeit √ºber die Zeit f√ºr alle Accounts. Vereinzelte russische Tweets in dominant englischen Accounts und anders herum, aber keine systemischen Ver√§nderungen sichtbar. Zus√§tzlich zeigt sich, dass viele Accounts nur f√ºr vergleichsweise kurze Zeit aktiv waren. Auch scheint immer wieder ein kleiner Anzeigefehler aufzutauchen, dieser wirkt sich aber bei genauerer Betrachtung nicht wirklich auf die sichtbaren Ergebnisse aus.


### Tweets vs. Retweets

#Zwei Graphen: Tweets/Retweets vs. Followerzahl, Tweets/Retweets vs. Tweetzahl


# Cleanup Data Sets ----

### Sprache: Da es hier um die Beeinflussung der USA gehen soll, sind nur englischsprachige Tweets von Interesse - Der ANteil von Amerikanern, die russisch, ukrainisch oder eine der anderen Sprachen beherrschen und auf zuf√§llig aufauchende Tweets in diesen Sprachen reagieren sollte nicht ausreichen, um einen bedeutenden Einfluss zu entwickeln.
tweets_eng <- tweets %>% filter(tweet_language %in% c("en"))
rm(tweets)

### Entfernung nicht ber√ºcksichtigter Informationen

# Retweets: Retweets sind zwar f√ºr eine Netzwerkanalyse ineressant, f√ºr die hier im folgenden angewendete Sprachprozessierung jedoch nicht wirklich hilfreich, da ein einzelner Tweet unter Umst√§nden durch Retweets mehrere hundert Male im Datensatz vorkommen und so die Klassifizierung beeinflussen k√∂nnten.
tweets_eng <- tweets_eng %>% filter(is_retweet == FALSE)

# Getaggte Nutzer: Viele der Tweets taggen andere Nutzer per @NUTERNAME. Da diese Information auch √ºber die Variable "user_mentions" in den Daten vorhanden ist und die Nutzernamen unter Umst√§nden die Textanalyse des Topic Models beeinflussen, werden sie zu Beginn entfernt. Auch Hashtag-Symbole k√∂nnen entfernt werden, da verwendete Hashtags separat in einer eigenen Variable getrackt werden.
tweets_eng$tweet_text <- gsub("@[a-zA-Z0-9_]*", "", tweets_eng$tweet_text)
tweets_eng$tweet_text <- gsub("#", "", tweets_eng$tweet_text)

### Verwendete Medien: Viele der Nutzer verkn√ºpfen ihre Posts mit Bild- oder Videomedien oder Links zu anderen Webinhalten. Diese werden innerhalb des Tweet-Textes als abgek√ºrzter Link (https://t.co/...) dargestellt. Da diese Medien und externen Verlinkungen bei der hier durchgef√ºhrten Analyse nicht beachtet werden, k√∂nnen sie entfernt werden.
tweets_eng$tweet_text <- gsub("https?://t.co/[a-zA-Z0-9]*", "", tweets_eng$tweet_text)

### Entfernung unn√∂tiger Klammern und leerer Eintr√§ge in bestimmten Spalten
tweets_eng$hashtags <- str_replace_all(tweets_eng$hashtags, "\\[", "")
tweets_eng$hashtags <- str_replace_all(tweets_eng$hashtags, "\\]", "")
tweets_eng$hashtags[tweets_eng$hashtags==""] <- NA

tweets_eng$urls <- str_replace_all(tweets_eng$urls, "\\[", "")
tweets_eng$urls <- str_replace_all(tweets_eng$urls, "\\]", "")
tweets_eng$urls[tweets_eng$urls==""] <- NA

tweets_eng$user_mentions <- str_replace_all(tweets_eng$user_mentions, "\\[", "")
tweets_eng$user_mentions <- str_replace_all(tweets_eng$user_mentions, "\\]", "")
tweets_eng$user_mentions[tweets_eng$user_mentions==""] <- NA

### Emoji
# Viele der Tweets beinhalten Emoji. Diese k√∂nnen vom stm-Textprozessor nicht bearbeitet werden, da sie zwar technisch als Zahlen- und Buchstabenkombinationen angegeben werden, eine korrekte Verarbeitung jedoch nicht gew√§hrleistet werden kann. Zus√§tzlich dazu ist es in vielen Tweets der Fall, dass Emoji untereinander bzw. Emoji und tats√§chliche Worte nicht durch Leerstelen getrennt werden. Diese Tatsache f√ºhrt dazu, dass der Gesamtverbund aus Emoji und Wort als Texteinheit etabliert wird und somit beispielsweise "‚òëÔ∏èwort" und "wort" als grundverschiedene Einheiten erfasst werden. Das f√ºhrt dazu, dass beispielsweise ein Topic-definierendes Wort ohne die Entfernung der Emoji "üí•eraseobamaÔøΩÔøΩÔøΩareÔøΩÔøΩÔøΩ" war. Eine Umbenennung der Emoji in Text war demnach eindeutig vonn√∂ten.
#Um dies zu beheben wurde auf Basis der offiziellen Emoji-Liste des Unicode-Konsortiums (https://www.unicode.org/emoji/charts/full-emoji-list.html, aufgerufen und erstellt am 30.03.2020) ein Datensatz erstellt, der die Emojinummer, das entsprechende Browser-Emoji und den jeweiligen offiziellen Kurznamen sowie die Anzahl der f√ºr jedes Emoji verwendeten Symbole beinhaltet. Diese Kurznamen wurden als Grundlage f√ºr die Text-Ersetzungen genommen. Die Voranstellung von "emoj_" an jeden der Begriffe sorgt dabei daf√ºr, dass jedes Emoji auch in Textform klar erkennbar bleibt. Die Entfernung jeglicher Leerstellen und Sonderzeichen sorgt daf√ºr, dass jedes Emoji als ein einzelnes Wort behandelt wird.
emoji <- read_csv2("Other Files/emoji-list.txt", col_names = T, col_types = cols(code = col_character(), Replace = col_character()), locale = locale(encoding = "UTF-8"))
#Hinzuf√ºgen einer Leerstelle, um Emoji voneinander zu trennen, sollten mehrere direkt aufeinander folgen
emoji$Replace <- paste(" ", emoji$Replace, " ")
for (i in seq(1,length(emoji$Replace))){tweets_eng$tweet_text <- gsub(emoji$code[i], emoji$Replace[i], tweets_eng$tweet_text)}
# ACHTUNG: Berechnete Laufzeit: Mehrere Stunden  (1809 Loop-Iterationen √ºber 2 Mio. Strings mit variablen L√§ngen)!
tweets_eng$tweet_text <- gsub("  ", " ", tweets_eng$tweet_text)
# Entfernung von m√∂glichen mehrfachen Leerstellen, um eventuell m√∂glichen Problemen zuvorzukommen.
rm(i, emoji)

# write_csv(tweets_eng, file.path("Twitter Data/tweets_en-cleaned.csv"), na = "NA", append = FALSE, col_names = T, quote_escape = "double")
# DATEIEN LADEN: tweets_eng <- read_csv("Twitter Data/tweets_en-cleaned.csv", col_types = cols(tweetid = col_character(), retweet_tweetid = col_character(), in_reply_to_tweetid = col_character(), latitude = col_factor(), longitude = col_factor(), poll_choices = col_character()))

### Das Problem der Duplikate ----
?duplicated
duplicates <- tweets_eng[which(base::duplicated(tweets_eng$tweet_text)), ]
duplicates$tweet_text[1:10]
tweets_clean <- tweets_eng[which(!(duplicated(tweets_eng$tweet_text))), ]
# Auch ohne Retweets schaffen es eine bedeutende Menge an Tweets, mehrfach in den Daten aufzuauchen, da sie wortgleich mehrfach gepostet wurden. Zwar lassen sich diese relativ simpel entfernen, aber es finden sich zweifellos auch wort√§hnliche Tweets bzw. Tweets, die sich nur durch das Erw√§hnen bestimmter Namen unterscheiden, und somit von dieser Filterung nicht erfasst werden w√ºrden. Zwar w√§re es f√ºr die Einheitlichkeit der Daten am Besten, diese Duplikate alle zu belassen, da dann aber die einzelnen Topics sehr von Formulierungen dominiert und wenig aussagekr√§ftig sein w√ºrden, wird dieser Filterung durchgef√ºhrt. Es muss einfach im Hinterkopf behalten werden, dass solche "unperfekten" Duplikate noch in den Daten vorhanden sein k√∂nnten.
rm(tweets_eng)


# STM - Vorbereitung ----

toks <- quanteda::tokens(tweets_clean$tweet_text,
                         remove_symbols = TRUE,
                         remove_separators = TRUE,
                         remove_punct = TRUE)

toks <- tokens_remove(tokens_tolower(toks), c(stopwords("en"), "will", "can", "says", "get", "say", "go"))
toks <- tokens_wordstem(toks)
dtm <- dfm(toks)
dtm <- dfm_trim(dtm, min_docfreq = 15)
#dtm 

docvars(dtm, "date") <- tweets_clean$tweet_time
docvars(dtm, "quotecount") <- tweets_clean$quote_count
docvars(dtm, "replycount") <- tweets_clean$reply_count
docvars(dtm, "likecount") <- tweets_clean$like_count
docvars(dtm, "retweetcount") <- tweets_clean$retweet_count

stm_dtm <- convert(dtm, to = "stm")
# Durch das Entfernen von Stopwords werden ca. 2.000 der 1,3m Tweets leer (""). Diese k√∂nnen nicht f√ºr weitere Analysen verwendet werden und werden hiermit entfernt.

used_documents <- names(stm_dtm$documents)
used_documents <- used_documents %>% gsub("^text", "", .) %>% as.integer(.)

# save(stm_dtm, file = "Saved Files/stm_dtm.RData")
# DATEN LADEN: load("Saved Files/stm_dtm.RData")
rm(dtm, toks)


# STM - Suche nach K ----

select_k <- searchK(stm_dtm$documents, stm_dtm$vocab, data = stm_dtm$meta,
                    K = seq(10, 110, by = 10),
                    prevalence =~ s(date) + quotecount + replycount + likecount + retweetcount,
                    init.type = "Spectral", max.em.its = 10, seed = 2020)

# save(select_k, file = "Saved Files/selectK.RData")
# DATEN LADEN: load("Saved Files/selectK.RData")
plot(select_k)
selectk_df <- data.frame(K = unlist(select_k$results$K), exclus = unlist(select_k$results$exclus), semcoh = unlist(select_k$results$semcoh),
                         heldout = unlist(select_k$results$heldout), residual = unlist(select_k$results$residual),
                         bound = unlist(select_k$results$bound), lbound = unlist(select_k$results$lbound), em.its = unlist(select_k$results$em.its))
selectk_df %>% select(K, semcoh, exclus, heldout, em.its) %>%
  pivot_longer(-K, names_to = "measure", values_to = "value") %>%
  ggplot(aes(x = K, y = value, group = measure, color = measure)) +
  geom_line() +  facet_wrap(.~measure, scale = "free", ncol = 2) +
  labs(y = "", title = "Exklusivit√§t und semantische Koh√§renz f√ºr K Topics", subtitle = "Bei einem Test mit max. 10 Iterationen") +
  theme_minimal() +
  theme(legend.position="none")
# Residuen-Spike bei 100 Topics, lbound-Minimum bei 90 Topics, Iteraionen: Ab 70 Topics Konvergenz bei unter 10 Iterationen, Exklusivit√§t: Plateau ab ca. 60 Topics mit lok. Minimum bei 80, Koh√§renz-Beuge ab 70  Topics, Verbesserung der Heldout-Likelihood bei 80 Topics, Plateau bei 90/100 Topics -> 90 Topics erscheinen als beste Wahl


# STM - Interpretation ----

stm_model_90 <- stm(stm_dtm$documents, stm_dtm$vocab, data = stm_dtm$meta,
                    K = 90,
                    prevalence =~ s(date) + quotecount + replycount + likecount + retweetcount,
                    init.type = "Spectral", max.em.its = 75, seed = 2020)
# save(stm_model_90, file = "Saved Files/stm_mod_90.RData")
# DATEN LADEN: load("Saved Files/stm_mod_90.RData")

plot(stm_model_90, type = "summary", xlim = c(0, 0.2), n = 5)
# Aufgrund der gro√üen Anzahl an Topics ist auch dies ein Plot, der vermutlich nur als abgespeicherte Datei betrachtet werden kann. Abmessungen von 15 x 8 in werden empfohlen.

# Manuelle Kodierung der Topics basierend auf zentralen Worten (prob ‚âô Wahrscheinlichkeit und frex ‚âô Exklusivit√§t zu Topic) sowie Top-Tweets des jeweiligen Topics, um Kategorisierung vornehmen zu k√∂nnen.

labels <- labelTopics(stm_model_90, topics = 90, n = 10)
prob <- list()
frex <- list()
for(i in c(1:90)){
  prob[[i]] <- paste(labels$prob[i,], collapse = ' ')
  frex[[i]] <- paste(labels$frex[i,], collapse = ' ')
}
labels_df <- data.frame(Prob = unlist(prob), Frex = unlist(frex), Topics = 1:90)
rm(labels, prob, frex, i)

# Kodierungsregeln:
# - 3 Hauptkategorien: News, Person, Spam
#   -> News: Sachlich formulierte S√§tze zu aktuellem Geschehen, wie sie sich bei Tweets von Nachrichtenorganisationen zu neuen Themen finden k√∂nnten. Das hei√üt nicht, dass alle diese Tweets tats√§chlich von diesen Organisationen kommen, nur, dass keine Wertung aus dem Tweet klar ersichtlich wird.
#   -> Spam: Tweets, die "normale Menschen" vermutlich nicht posten w√ºrden: Entweder, weil der Tweet selbst durch √ºberm√§√üiges Taggen anderer Nutzer auff√§llt, eine bedeutende Menge an Emoji beinhaltet (wobei hier der genaue Umbruchpunkt rein subjektiv ist und in meinen Augen  zwischen 4-5 liegt) oder √§hnliches Verhalten an den Tag legt. Auf Topic-Ebene: Tweets, die zwar individuell "normal" erscheinen, sich aber √ºber mehrere Tweets hinweg deutlich in Formulierungen und Satzstrukturen √§hneln, sodass von einer gemeinsamen Quelle mit speziefischem Ziel ausgegangen werden kann. Bei Spam zu politischen Themen wird zudem eine vermutende Eingruppierung in rechte / linke Ideen und Positionen unterschieden.
#   -> Person: Tweets √ºber private Angelegenheiten, Zitate, Nachrichtenvermittlung mit wertender Einordnung; Tweets wie sie ein "normaler Mensch" schreiben k√∂nnte.
# - Zus√§tzlich zu dieser groben Einteilung werden auch die jeweils behandelten Themenkomplexe (z.B. Sportnachrichten, Spam-Werbung f√ºr ein bestimmtes Produkt, Pers√∂nliche Tweets zu Workoutroutinen, ...) erfasst und aufgelistet, um deren Anteil und Verteilung untersuchen zu k√∂nnen.
# - Auch bestimmte Worte, die sich durch die Tweets ziehen, werden festgehalten, da diese die einzelnen Topics erkl√§ren k√∂nnen. Sie werden mit Anf√ºhrungszeichen als vorkommende Worte im Gegensatz zu abgeleiteten √úberschriften markiert.

# Um einer bestimmten Hauptkategorie zugeordnet zu werden, m√ºssen mindestens 6 der top 20 Tweets des jeweiligen Topics der Kategorie entstammen.
# Um ein Wort zugeordnet zu bekommen, muss es entweder in mindestens 6 der top 20 Tweets oder in den Prob-/Frex-Listen des jeweiligen Topics vorkommen.
# -> 6/20, damit die Kodierung die M√∂glichkeit zul√§sst, bei Topics ohne klaren Fokus alle drei Labels anbringen zu k√∂nnen, ohne die Kodierregeln zu brechen. In F√§llen, bei denen die Zuordnung knapp an dieser Grenze scheiterte (Topics 21, 25, 59, 61), oder bei denen eine genauere Betrachtung vonn√∂ten war, um die Inhalte einzuordnen (Topics 30, 52, ) wurden die top 30 Tweets betrachtet und mit 9 Tweets als Schwellenwert gearbeitet.

top <- 90 #Zu betrachtendes Topic
{
  print(labels_df[top, 1])
  print(labels_df[top, 2])
  thought <- findThoughts(stm_model_90, n = 20, topics = top, text = tweets_clean$tweet_text[used_documents])$docs[[1]]
  plotQuote(thought, width = 90, main = paste("Topic", top, sep = " "))
}
# Aufgrund der L√§nge einiger Spam-Tweets (durch das Ausschreiben der Emoji) kann es hilfreich sein, die Grafiken abzuspeichern und dann zu betrachten. Ein PNG mit einer Hohe von 2000 Pixeln sollte dabei ausreichen.

# Die Topics mit dem gr√∂√üten erwarteten Anteil drehen sich um lokale Verbrechen (Top. 68, Platz 1), Sport (Top. 71, Platz 2) und Gerichte und -entscheidungen (Top. 66, Platz 3). ALl diese Topics wurden als "News" deklariert. Das erste "Person"-Topic liegt auf Platz 4 (Top. 2, "Workout"), das erste Spam-Topic auf Platz 9 (Top. 39, Ukraine-Verschw√∂rungstheorie).
rm(thought, top)


### Topic-Korrelation
label_csv <- read.csv("Other Files/STM_TopicLabels.csv", sep = ';', stringsAsFactors = F)

corr <- cor(stm_model_90$theta[,1:90])
dissim <- 1 - corr
dist_mat <- as.dist(dissim)
colnames(corr) <- paste(label_csv$Number, label_csv$Label, sep = ". ")
rownames(corr) <- paste(label_csv$Number, label_csv$Label, sep = ". ")
diag(corr) <- 0
min(corr)
max(corr)
corrplot(corr, order = "hclust", hclust.method = "complete",
         cl.lim = c(-.33,.33), is.corr=FALSE, diag = F,
         insig = "blank", method="color",
         addrect = 10, cl.pos = "b", tl.cex = .45,
         tl.col = "black",rect.col = "black",
         #col = rev(colorRamp("blue2red")))
         col = c(rev(gray.colors(120))[1:43], "white", rev(gray.colors(120))[78:120]))
# F√ºr bessere Betrachtung: Abspeicherung als .pdf (10" x 10") wird empfohlen.
rm(dissim, dist_mat)


### Topic-Verteilungen

#W√∂chentlich gemittelt
dates <- paste(year(stm_dtm$meta$date), "-", isoweek(stm_dtm$meta$date), sep = "")
for (i in 1:length(dates)) {
  if (nchar(dates[i]) == 6) {
    stri_sub(dates[i], 6, 5) <- 0
  }
}
topic_times <- data.frame(dates, stm_model_90$theta)
counts <- topic_times %>% count(dates)
colnames(topic_times) <- c("date", paste("topic_",1:90, sep = ""))
topic_times <- aggregate(.~date, FUN = mean, data = topic_times)
topic_times <- topic_times %>% mutate(count = counts[,2])

topic_times.long <- reshape2::melt(topic_times, id.vars = c("date", "count"))
ggplot(topic_times.long, aes(x = date, y = value * count, group = variable, color = variable)) +
  geom_line() + theme(legend.position = "bottom") + 
  geom_line(aes(x = date, y = count), color = "black") +
  scale_y_sqrt() +
  labs(title = "Topic-Anteile gemittelt nach Woche", y = "Anzahl an Tweets", x = "Kalenderwoche") +
  theme(axis.text.x = element_text(angle = 90), legend.position = "none")

# Gruppiert nach Themenkomplex
topic_grp <- topic_times %>% select(-c(date, count))
topic_grp <- as.data.frame(t(topic_grp))
topic_grp <- topic_grp %>% mutate(group = label_csv$Group)
topic_grp <- aggregate(.~group, FUN = sum, data = topic_grp)
grp_names <- topic_grp[,1]
topic_grp <- select(topic_grp,-c(group))
topic_grp <- as.data.frame(t(topic_grp))
topic_grp <- topic_grp %>% mutate(date = topic_times$date, count = topic_times$count)
colnames(topic_grp) <- c(grp_names, "date", "count")

topic_grp.long <- reshape2::melt(topic_grp, id.vars = c("date", "count"))
ggplot(topic_grp.long, aes(x = date, y = value, group = variable, color = variable)) + 
  geom_line() + theme(legend.position = "bottom") + 
  labs(title = "Tweet-Kategorien gemittelt nach Woche", y = "Anteil", x = "Kalenderwoche") +
  theme(axis.text.x = element_text(angle = 90))
ggplot(topic_grp.long, aes(x = date, y = value, group = variable, fill = variable)) + 
  geom_bar(position="stack", stat="identity") + theme(legend.position = "bottom") + 
  labs(title = "Tweet-Kategorien gemittelt nach Woche", y = "Anteil", x = "Kalenderwoche") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))

